{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiviteshSoneji/JiviteshSoneji.github.io/blob/main/basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbbkME7AesNF",
        "outputId": "b56a6d0a-32c1-4a11-d6c0-056bcb5f5072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets \n",
        "import opendatasets as od\n",
        "\n",
        "!pip install pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgbCsSkJfjLy",
        "outputId": "7cc6c71f-6d4f-406f-abeb-a6f85ddf25dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: shrimayshah\n",
            "Your Kaggle Key: ··········\n",
            "Downloading siic-isic-224x224-images.zip to ./siic-isic-224x224-images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.27G/3.27G [02:11<00:00, 26.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/arroqc/siic-isic-224x224-images?resource=download\")\n",
        "\n",
        "#kaggle name : shrimayshah\n",
        "#kaggle key : 0e534d9f47a3d7170f64b594279ae133"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/muratkokludataset/pistachio-dataset\")\n",
        "\n",
        "#kaggle name : shrimayshah\n",
        "#kaggle key : 0e534d9f47a3d7170f64b594279ae133"
      ],
      "metadata": {
        "id": "bAc8apQVEwJU",
        "outputId": "4390ce84-2bfb-409b-9144-b07d9fa85f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: shrimayshah\n",
            "Your Kaggle Key: ··········\n",
            "Downloading pistachio-dataset.zip to ./pistachio-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.82M/1.82M [00:00<00:00, 7.96MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3RjQt4MfoUL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "#make sure to mount drive from left panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH2vreH2i6lp",
        "outputId": "c04ae0eb-ea47-4ab7-e967-272498cefdcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4VHZFP6fz3U",
        "outputId": "1caa5e51-009a-48c8-fb6a-20d5865363ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_name', 'patient_id', 'sex', 'age_approx',\n",
            "       'anatom_site_general_challenge', 'diagnosis', 'benign_malignant',\n",
            "       'target', 'image_path'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "# Print column names\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufa6-Xwff2pY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "ee8d224f-a4e1-456e-f569-d525167b1079"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5606ac73f651>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add new column to CSV file containing image file paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/siic-isic-224x224-images/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save updated CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5606ac73f651>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add new column to CSV file containing image file paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/siic-isic-224x224-images/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save updated CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load CSV file containing attributes\n",
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "# Add new column to CSV file containing image file paths\n",
        "data['image_path'] = data['image_name'].apply(lambda x: os.path.join('/content/siic-isic-224x224-images/train', x + '.png'))\n",
        "\n",
        "# Save updated CSV file\n",
        "data.to_csv('/content/drive/MyDrive/train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUSo-TFzf5Y9",
        "outputId": "e8296347-be16-46da-d41f-9c4514ab1ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_name', 'patient_id', 'sex', 'age_approx',\n",
            "       'anatom_site_general_challenge', 'diagnosis', 'benign_malignant',\n",
            "       'target', 'image_path'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "\n",
        "# Print column names\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfoioBP4f9Hy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfNn_xmYf_ZS"
      },
      "outputs": [],
      "source": [
        "# Define data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeeHv4JfgCAh"
      },
      "outputs": [],
      "source": [
        "# Load dataset and corresponding attributes from CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "dataset = []\n",
        "\n",
        "l = len(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxpFlGYPgEeY"
      },
      "outputs": [],
      "source": [
        "#reading datapath and creating new dataset\n",
        "\n",
        "for i in range(l):\n",
        "  if i<15:\n",
        "    img_path = data.iloc[i]['image_path']\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = transform(img)\n",
        "    label = data.iloc[i]['target']\n",
        "   # label2 = data.iloc[i]['age_approx']\n",
        "   # label3 = data.iloc[i]['sex']\n",
        "    dataset.append((img_tensor, label))#, label2, label3))\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSY_qUn6gG2r"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorchcv "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe_Uxn_reDYw",
        "outputId": "591eeef1-9bba-4a82-9d4e-281a1bfd1872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorchcv\n",
            "  Downloading pytorchcv-0.0.67-py2.py3-none-any.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.4/532.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorchcv) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorchcv) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (3.4)\n",
            "Installing collected packages: pytorchcv\n",
            "Successfully installed pytorchcv-0.0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pytorchcv "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa-zQ8AZpw-I",
        "outputId": "8d3993c0-b567-4231-db12-408c339132e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorchcv in /usr/local/lib/python3.10/dist-packages (0.0.67)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorchcv) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorchcv) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorchcv) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQkEudYLsxTd",
        "outputId": "88ae4c8f-0a34-4765-ca9e-48166b7506e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet_pytorch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=def2bc39136882574f4b0fc08c16fad7d0b7a05f84294d02b229dae120ae96db\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING THE EFFICIENTNET-B6 MODEL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# # Load the data\n",
        "# train_data = torchvision.datasets.ImageFolder(root='/content/siic-isic-224x224-images/train', transform=transform)\n",
        "# train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "# test_data = torchvision.datasets.ImageFolder(root='/content/siic-isic-224x224-images/test', transform=transform)\n",
        "# test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# # Define the model architecture\n",
        "# model = ptcv_get_model(\"efficientnet_b6\", pretrained=True)\n",
        "# model.head.fc = nn.Linear(model.head.fc.in_features, 1)\n",
        "\n",
        "# # Define the loss function and optimizer\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Define the model architecture\n",
        "model = EfficientNet.from_pretrained('efficientnet-b6')\n",
        "num_ftrs = model._fc.in_features\n",
        "model._fc = nn.Linear(num_ftrs, 1)  # Modify the last fully connected layer for binary classification\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.5f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQoQM7sNa0cl",
        "outputId": "427339f9-0f53-4987-e6e3-7bafc429651a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b6-c76e70fd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b6-c76e70fd.pth\n",
            "100%|██████████| 165M/165M [00:03<00:00, 54.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b6\n",
            "Epoch 1 loss: 0.71183\n",
            "Epoch 2 loss: 0.65926\n",
            "Epoch 3 loss: 0.60815\n",
            "Epoch 4 loss: 0.52821\n",
            "Epoch 5 loss: 0.43116\n",
            "Epoch 6 loss: 0.33799\n",
            "Epoch 7 loss: 0.28931\n",
            "Epoch 8 loss: 0.24392\n",
            "Epoch 9 loss: 0.15424\n",
            "Epoch 10 loss: 0.11503\n",
            "Accuracy on test set: 100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS BLOCK GIVES ACCURACY ON UNIFIED DARK SKIN TONE DATASET for EfficientNet-B6\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the preprocessing steps for the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, .225]) ])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in test_data.iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx3_N2mqvSfB",
        "outputId": "c8373fb2-aa95-483e-ecb1-7dcb4c7ad1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 73.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Accuracy: {:.20f}%'.format(accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfRM0jKUyLQn",
        "outputId": "c7d5aeeb-22ae-43db-c424-1afbfb3ae670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 73.93292682926829684220%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oG7L1Q5gJfH",
        "outputId": "b13c99cb-96ca-42b3-b321-8a1570f99445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 0.15418\n",
            "Epoch 2 loss: 0.08046\n",
            "Epoch 3 loss: 0.08158\n",
            "Epoch 4 loss: 0.07905\n",
            "Epoch 5 loss: 0.08929\n",
            "Epoch 6 loss: 0.08902\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# Define the model architecture\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.5f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct / total))\n",
        "\n",
        "#this code takes an hour to run, and gave accuracy as 98% previously, for 1<2000, hence proving that model has been trained efficiently\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihx1EnwKgMm_",
        "outputId": "80f61010-dd35-4909-a3a0-dfe3e7d048df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'DDI_ID', 'DDI_file', 'skin_tone', 'malignant', 'disease',\n",
            "       'image_path', 'target'],\n",
            "      dtype='object')\n",
            "   Unnamed: 0  DDI_ID    DDI_file  skin_tone  malignant  \\\n",
            "0           0       1  000001.png         56       True   \n",
            "1           1       2  000002.png         56       True   \n",
            "2           2       3  000003.png         56       True   \n",
            "3           3       4  000004.png         56       True   \n",
            "4           4       5  000005.png         12       True   \n",
            "\n",
            "                           disease  \\\n",
            "0                 melanoma-in-situ   \n",
            "1                 melanoma-in-situ   \n",
            "2                mycosis-fungoides   \n",
            "3  squamous-cell-carcinoma-in-situ   \n",
            "4             basal-cell-carcinoma   \n",
            "\n",
            "                                         image_path  target  \n",
            "0  /content/drive/MyDrive/test2/class_t2/000001.png       1  \n",
            "1  /content/drive/MyDrive/test2/class_t2/000002.png       1  \n",
            "2  /content/drive/MyDrive/test2/class_t2/000003.png       1  \n",
            "3  /content/drive/MyDrive/test2/class_t2/000004.png       1  \n",
            "4  /content/drive/MyDrive/test2/class_t2/000005.png       1  \n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv(\"/content/drive/MyDrive/test2/ddi_metadata.csv\")\n",
        "\n",
        "# Add new column to CSV file containing image file paths\n",
        "test_data['image_path'] = test_data['DDI_file'].apply(lambda x: os.path.join('/content/drive/MyDrive/test2/class_t2', x))\n",
        "\n",
        "# Save updated CSV file\n",
        "test_data.to_csv('/content/drive/MyDrive/test2/ddi_metadata.csv', index=False)\n",
        "\n",
        "print(test_data.columns)\n",
        "\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L52CJ_I0qR4E"
      },
      "outputs": [],
      "source": [
        "#THIS BLOCK GIVES ACCURACY ON UNIFIED DARK SKIN TONE DATASET\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the preprocessing steps for the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, .225]) ])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in test_data.iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDTvr4VgRV6c"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset based on skin_tone value\n",
        "skin_tone_classes = {}\n",
        "for skin_tone_value in test_data['skin_tone'].unique():\n",
        "    skin_tone_classes[skin_tone_value] = test_data[test_data['skin_tone'] == skin_tone_value]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ApU2L7tUACp"
      },
      "outputs": [],
      "source": [
        "skin_tone_classes.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIDZUWD9UDXt"
      },
      "outputs": [],
      "source": [
        "#EVALUATION ON FIRST SKIN TONE\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in skin_tone_classes[56].iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wng7-7MIULfl"
      },
      "outputs": [],
      "source": [
        "#EVALUATION ON SECOND SKIN TONE\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in skin_tone_classes[34].iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqn2YZloURqh"
      },
      "outputs": [],
      "source": [
        "#EVALUATION ON THIRD SKIN TONE\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in skin_tone_classes[12].iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cImoL7zxZ1Qb"
      },
      "outputs": [],
      "source": [
        "# METRICS FOR ALL DARK SKINTONES\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "# Define the preprocessing steps for the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, .225])\n",
        "])\n",
        "\n",
        "# Set the model evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in test_data.iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "    \n",
        "    # Append the true and predicted labels for computing metrics\n",
        "    y_true.append(row['target'])\n",
        "    y_pred.append(predicted.item())\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
        "print('ROC AUC {:.2f}'.format(roc_auc))\n",
        "print('Precision: {:.2f}'.format(precision))\n",
        "print('Sensitivity/Recall: {:.2f}'.format(recall))\n",
        "print('Specificity: {:.2f}'.format(specificity))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9A7yLYpbss_"
      },
      "outputs": [],
      "source": [
        "ans = [{'True', 'False'}]\n",
        "# Calculate the evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_pred, average = 'macro', sample_weight=None, max_fpr=1.0, multi_class='ovo', labels= ans)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
        "print('ROC AUC {:.2f}'.format(roc_auc))\n",
        "print('Precision: {:.2f}'.format(precision))\n",
        "print('Sensitivity/Recall: {:.2f}'.format(recall))\n",
        "print('Specificity: {:.2f}'.format(specificity))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpS4rWzIY0Ja"
      },
      "outputs": [],
      "source": [
        "# METRICS FOR FiRST DARK SKIN TONE\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in skin_tone_classes[56].iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "    \n",
        "    # Append the true and predicted labels for computing metrics\n",
        "    y_true.append(row['target'])\n",
        "    y_pred.append(predicted.item())\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
        "print('ROC AUC {:.2f}'.format(roc_auc))\n",
        "print('Precision: {:.2f}'.format(precision))\n",
        "print('Sensitivity/Recall: {:.2f}'.format(recall))\n",
        "print('Specificity: {:.2f}'.format(specificity))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDqVh3rQfoXh"
      },
      "outputs": [],
      "source": [
        "#TRAINING RESNET WITHOUT IMAGENET PRETRAINED\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the model_un architecture\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model_un = ResNet18()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model_un.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model_un\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_un(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.5f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "\n",
        "# Test the model_un\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model_un(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEcc4mXchKCf"
      },
      "outputs": [],
      "source": [
        "#THIS BLOCK GIVES ACCURACY ON UNIFIED DARK SKIN TONE DATASET\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the preprocessing steps for the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, .225]) ])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_un.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in test_data.iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_un(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.8f}%'.format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbu8zwhFhJ-5"
      },
      "outputs": [],
      "source": [
        "#TRAINING DENSENET MODEL WITH IMAGENET PRETRAINED\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained DenseNet model\n",
        "model_dense = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for param in model_dense.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last fully connected layer with a new one\n",
        "num_ftrs = model_dense.classifier.in_features\n",
        "model_dense.classifier = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model_dense.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_dense(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.5f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model_dense(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m8yrTAlktfP"
      },
      "outputs": [],
      "source": [
        "#THIS BLOCK GIVES ACCURACY ON UNIFIED DARK SKIN TONE DATASET, USING DENSENET WITH IMAGENET PRETRAINED\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the preprocessing steps for the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, .225]) ])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_dense.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in test_data.iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_dense(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.8f}%'.format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2FRDVXLmRaC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the DenseNet architecture\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(64),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(128),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(256),\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.BatchNorm2d(512),\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.classifier = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model_dense_un = DenseNet()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model_dense_un.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_dense_un(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch %d loss: %.5f' % (epoch + 1, running_loss / len(train_loader)))\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model_dense_un(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKYTk3L_pqnX"
      },
      "outputs": [],
      "source": [
        "#THIS BLOCK GIVES ACCURACY ON UNIFIED DARK SKIN TONE DATASET, USING DENSENET WITH IMAGENET PRETRAINED\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the preprocessing steps for the images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, .225]) ])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model_dense_un.eval()\n",
        "\n",
        "# Initialize variables to keep track of the number of correct and total predictions\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Loop over the images in the dataset and make predictions\n",
        "for i, row in test_data.iterrows():\n",
        "    # Load the image and preprocess it\n",
        "    img = Image.open(row['image_path']).convert('RGB')\n",
        "    # print(img.size)\n",
        "    img = preprocess(img)\n",
        "    \n",
        "    # Add a batch dimension to the image\n",
        "    img = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_dense_un(img)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Compare the predicted class label with the true class label and update the counters\n",
        "    if predicted.item() == row['target']:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = correct / total\n",
        "print('Accuracy: {:.8f}%'.format(accuracy * 100))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}